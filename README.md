# ğŸ“˜ ragas-openai-pytest-llm-evaluator

A powerful **Pytest-based evaluation framework** for analyzing Retrieval-Augmented Generation (RAG) pipelines and Large Language Models (LLMs).  
This tool enables robust testing of **retrieval**, **augmentation**, and **generation** stages using industry-leading metrics from **RAGAS**, **LangChain**, and **OpenAI**.

---

# ğŸ“‘ Contents
1. [Overview](#overview)
2. [Features](#features)
3. [Tech Stack](#tech-stack)
4. [Evaluation Metrics](#evaluation-metrics)
5. [Dataset Generation](#dataset-generation)
6. [Repository Structure](#repository-structure)
7. [Execution Commands](#execution-commands)
8. [SonarQube Integration](#sonarqube-integration)
9. [Jenkins Pipeline (CI/CD)](#jenkins-pipeline-cicd)

---

# ğŸ’¡ Overview

This framework allows developers and researchers to perform **end-to-end evaluation** of RAG-based pipelines, ensuring quality across the following stages:

- **Retrieval Evaluation**
- **Augmentation Evaluation**
- **Generation Evaluation**

It integrates seamlessly with:

- **RAGAS** â€” Metric evaluation  
- **LangChain** â€” LLM orchestration  
- **OpenAI** â€” Model execution  
- **Allure** â€” Test reporting  

### ğŸ“Š RAG Evaluation Flow

![RAG Pipeline](utilities/images/img.png)

---

# ğŸš€ Features

### 1ï¸âƒ£ Multi-Stage Testing  
Evaluate retrieval, augmentation, and generation independently.

### 2ï¸âƒ£ Rich Metric Coverage  
Includes all major RAGAS **singleton** and **multi-turn** metrics.

### 3ï¸âƒ£ Multi-Turn Conversation Support  
Evaluate consistency, context retention, and topic adherence.

### 4ï¸âƒ£ Synthetic Dataset Generation  
Generate both **single-turn** and **multi-turn** datasets easily.

### 5ï¸âƒ£ CI/CD Friendly  
Supports **SonarQube** code quality and **Jenkins** automation.

---

# âš™ï¸ Tech Stack

| Component | Purpose |
|----------|---------|
| **Python** | Core implementation |
| **Pytest** | Test execution |
| **LangChain** | LLM orchestration |
| **OpenAI** | Model integration |
| **RAGAS Library** | Evaluation metrics |
| **Allure** | Test reporting |

---

# ğŸ“Š Evaluation Metrics

## ğŸ”¹ Single-turn Metrics
- Faithfulness  
- Context Precision  
- Context Recall  
- Answer Relevancy  
- Factual Correctness  
- Rubric Score  

## ğŸ”¸ Multi-Turn Metrics
- Aspect Critic  
- Topic Adherence Score  
- Rubric Score  
- Conversational Memory Score  

---

# ğŸ§ª Dataset Generation

Supports creation of structured datasets for evaluation:

### **1. Single-Turn Dataset**
- Q/A pairs  
- Ground truth  
- Knowledge chunks  

### **2. Multi-Turn Dataset**
- Conversation flows  
- Follow-up questions  
- Context evolution  

---
# âœ… How to Use

## 1. Clone the Repository

``` bash
git clone <your-repo-url>
cd ragas-openai-pytest
```

## 2. Configure Environment

Update the `.env` file:

    Update OPENAI_API_KEY, LANGCHAIN_API_KEY, SMTP_HOST, SMTP_PORT_NUMBER, EMAIL_SENDER, EMAIL_RECEIVER Details

## 3. Install Dependencies

``` bash
pip install -r requirements.txt
```

## 4. Run Tests

Run all evaluation tests: pytest

``` bash
pytest
```

Generate Allure test report:

``` bash
pytest --alluredir=reports/
```

# ğŸ“‚ Repository Structure
```
ragas-openai-pytest/
â”‚
â”œâ”€â”€ tests/                     # All pytest test cases
â”‚   â”œâ”€â”€ test_loyalty_tier_offers.py   # Singleton RAGAS tests
â”‚   â”œâ”€â”€ test_rest_assured.py          # Multi-turn tests
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ llm-base/                  # Core logic for dataset generation & evaluation
â”‚   â”œâ”€â”€ ragas_dataset_generator.py    # Create single-turn & multi-turn datasets
â”‚   â”œâ”€â”€ ragas_metrics_evaluator.py    # Evaluate various RAG metrics
â”‚
â”œâ”€â”€ utilities/                 # Helper utilities
â”‚   â”œâ”€â”€ assertions.py
â”‚   â”œâ”€â”€ email_reporter.py
â”‚   â”œâ”€â”€ ironman.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ generate-datasets/
â”‚   â”œâ”€â”€ generate_dataset.py           # Script to create datasets
â”‚
â”œâ”€â”€ dataset/                   # Pre-generated datasets
â”‚   â”œâ”€â”€ loyalty-tier-offers/
â”‚   â”‚   â”œâ”€â”€ singleturn_dataset.json
â”‚   â”‚   â””â”€â”€ multiturn_dataset.json
â”‚   â”œâ”€â”€ rest_assured/
â”‚   â”‚   â”œâ”€â”€ singleturn_dataset.json
â”‚   â”‚   â””â”€â”€ multiturn_dataset.json
â”‚
â”œâ”€â”€ feature_documents/         # Source documents for RAG
â”‚   â”œâ”€â”€ loyalty-tier-offers/
â”‚   â”œâ”€â”€ rest_assured/
â”‚
â”œâ”€â”€ configs/                   # Configurations for LLM/RAG
â”‚   â”œâ”€â”€ openai_config.yaml
â”‚   â”œâ”€â”€ rag_pipeline.yaml
â”‚   â””â”€â”€ environment.yaml
â”‚
â”œâ”€â”€ utilities/                 # Non-code assets
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ img.png            # Diagram used in README
â”‚   â””â”€â”€ logs/                  # Log files (optional)
â”‚
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ conftest.py
â”œâ”€â”€ .jenkins                   # Jenkins pipeline configs
â”œâ”€â”€ sonar-project.properties   # SonarQube configuration
â”œâ”€â”€ README.md                  # Project documentation
â””â”€â”€ .gitignore
```