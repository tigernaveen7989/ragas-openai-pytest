{"name": "test_top_adherence_score[get_multiturn_data1]", "status": "broken", "statusDetails": {"message": "TypeError: not all arguments converted during string formatting", "trace": "self = <tests.test_rest_assured.TestRestAssured object at 0x0000026B39F59FD0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess... resource usage to determine if the system meets performance requirements and can scale to the expected user volume.'])\nlogger = <Logger pytest_logger (DEBUG)>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n        logger.info(f\"Top Adherence Score: {score}\")\n>       self.assertions.assert_top_adherence(score=score, threshold=0.8)\n\ntest_rest_assured.py:46: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\utilities\\assertions.py:120: in assert_top_adherence\n    self.logger.info(\"Expected Threshold \", threshold, \"Score \", score)\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1539: in info\n    self._log(INFO, msg, args, **kwargs)\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1684: in _log\n    self.handle(record)\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1700: in handle\n    self.callHandlers(record)\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1762: in callHandlers\n    hdlr.handle(record)\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1028: in handle\n    self.emit(record)\n..\\.venv\\Lib\\site-packages\\_pytest\\logging.py:384: in emit\n    super().emit(record)\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1168: in emit\n    self.handleError(record)\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1160: in emit\n    msg = self.format(record)\n          ^^^^^^^^^^^^^^^^^^^\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:999: in format\n    return fmt.format(record)\n           ^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\_pytest\\logging.py:137: in format\n    return super().format(record)\n           ^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:703: in format\n    record.message = record.getMessage()\n                     ^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <LogRecord: pytest_logger, 20, C:\\Users\\SG0704235\\PycharmProjects\\ragas-openai-pytest\\utilities\\assertions.py, 120, \"Expected Threshold \">\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n    \n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n>           msg = msg % self.args\n                  ^^^^^^^^^^^^^^^\nE           TypeError: not all arguments converted during string formatting\n\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:392: TypeError"}, "description": "Validates that the model response remains top adherence to the reference context for rest assured", "steps": [{"name": "Calculate Top Adherence Score", "status": "passed", "attachments": [{"name": "User Input", "source": "8c473e60-4e51-4b6f-ba7a-00b17be164a2-attachment.txt", "type": "text/plain"}, {"name": "Chatbot Response", "source": "3bd93e9b-f0be-4b7d-9754-63e2f0aa4c44-attachment.txt", "type": "text/plain"}, {"name": "Individual Adherence Scores", "source": "50120a1c-0335-4f42-b1bf-5c4982548278-attachment.txt", "type": "text/plain"}, {"name": "Final Top Adherence Score", "source": "5cf849a4-84fe-4bb4-9996-bce9647edf01-attachment.txt", "type": "text/plain"}], "start": 1763188755163, "stop": 1763188760981}, {"name": "Top Adherence Score: 1.0", "status": "passed", "start": 1763188760981, "stop": 1763188760981}], "attachments": [{"name": "log", "source": "8e624e4a-ee9a-4288-9bf3-61f1e2f370c5-attachment.txt", "type": "text/plain"}, {"name": "stdout", "source": "7f1cf236-fba0-49fe-8ba5-8cd69deb9c75-attachment.txt", "type": "text/plain"}, {"name": "stderr", "source": "3d182c58-0d0b-4dcc-83d2-b838dc63ec52-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "get_multiturn_data", "value": "{'conversation': [{'role': 'human', 'content': 'What is performance testing?'}, {'role': 'ai', 'content': 'Performance testing is a type of non-functional testing that measures how an application performs under a specific workload. It checks parameters like speed, scalability, stability, and responsiveness.'}, {'role': 'human', 'content': 'What tools are commonly used for performance testing?'}, {'role': 'ai', 'content': 'Common tools include Apache JMeter, LoadRunner, Gatling, k6, and Locust. Among these, JMeter is widely used for open-source performance testing of web and API applications.'}, {'role': 'human', 'content': 'Can you explain how JMeter works?'}, {'role': 'ai', 'content': 'JMeter simulates multiple virtual users sending requests to a target server and measures how the system responds. It can test web applications, APIs, and databases by recording test plans, executing load scenarios, and collecting performance metrics like throughput, latency, and error rate.'}, {'role': 'human', 'content': 'How do you analyze JMeter test results?'}, {'role': 'ai', 'content': 'You can analyze JMeter results through listeners such as Summary Report, Aggregate Report, and Graph Results. Metrics like response time, error percentage, and throughput help identify performance bottlenecks and scalability issues.'}], 'reference_contexts': ['Performance testing evaluates the systemâ€™s responsiveness, stability, and scalability under load.', 'JMeter is an open-source tool that simulates concurrent users and measures performance metrics.', 'JMeter results can be analyzed through reports and graphs to find bottlenecks.'], 'reference': 'Performance testing ensures an application remains stable, scalable, and responsive under expected load. JMeter helps simulate concurrent user activity and analyze metrics such as throughput, latency, and error rate to identify performance issues.', 'synthesizer_name': 'multi_turn_performance_testing_jmeter'}"}], "start": 1763188755163, "stop": 1763188760986, "uuid": "2cf4be5d-2406-4780-8ee4-390cc4672a13", "historyId": "aad318962d31c615c5aaef4fa992b847", "testCaseId": "b483da4e4716765c33f1fead839d530b", "fullName": "tests.test_rest_assured.TestRestAssured#test_top_adherence_score", "labels": [{"name": "feature", "value": "Rest Assured"}, {"name": "story", "value": "Top Adherence Evaluation"}, {"name": "suite", "value": "Rest Assured Evaluation Suite"}, {"name": "tag", "value": "asyncio"}, {"name": "parentSuite", "value": "tests"}, {"name": "subSuite", "value": "TestRestAssured"}, {"name": "host", "value": "W4KPW724"}, {"name": "thread", "value": "21192-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.test_rest_assured"}], "titlePath": ["tests", "test_rest_assured.py", "TestRestAssured"]}