{"name": "test_faithfulness[get_singleturn_data6]", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ Faithfulness score too low: 0.0. Expected > 0.7", "trace": "self = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x000002259A3A8A40>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What should happen when adding seats or ancillaries to an existing order?', retrieved_con...prepared for a specific order with a specific FF# and tier level.', multi_responses=None, reference=None, rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\n\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response remains faithful to the reference context for loyalty tier offers.\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_faithfulness(self, get_llm_wrapper, get_singleturn_data, logger):\n        \"\"\"\n        Test to validate faithfulness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        score = await evaluator.get_faithfulness_score(get_singleturn_data)\n        logger.info(f\"Faithfulness Score: {score}\")\n    \n        # Validation threshold\n>       Assertions.assert_faithfulness(score)\n\ntest_loyalty_tier_offers.py:29: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nscore = 0.0, threshold = 0.7\n\n    @staticmethod\n    def assert_faithfulness(score: float, threshold: float = 0.7):\n        \"\"\"\n        Validate that the faithfulness score meets the minimum threshold.\n        \"\"\"\n        with allure.step(f\"Validate Faithfulness Score ≥ {threshold}\"):\n>           assert score > threshold, f\"❌ Faithfulness score too low: {score}. Expected > {threshold}\"\n                   ^^^^^^^^^^^^^^^^^\nE           AssertionError: ❌ Faithfulness score too low: 0.0. Expected > 0.7\n\n..\\utilities\\assertions.py:15: AssertionError"}, "description": "Validates that the model response remains faithful to the reference context for loyalty tier offers.", "steps": [{"name": "Calculate Faithfulness Score", "status": "passed", "attachments": [{"name": "User Input", "source": "2987ef8c-e9ed-4256-a7de-5906cc7cf894-attachment.txt", "type": "text/plain"}, {"name": "Retrieved Contexts", "source": "eb01fc2a-e819-4a8a-b0ec-01ce7e03600f-attachment.txt", "type": "text/plain"}, {"name": "Reference Response", "source": "27a6a067-d486-4fef-a155-2ce3d1f04452-attachment.txt", "type": "text/plain"}, {"name": "Faithfulness Score", "source": "0df85d73-c936-4e04-93e8-59ac92e77e11-attachment.txt", "type": "text/plain"}], "start": 1762675110281, "stop": 1762675123237}, {"name": "Faithfulness Score: 0.0", "status": "passed", "start": 1762675123237, "stop": 1762675123237}, {"name": "Validate Faithfulness Score ≥ 0.7", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ Faithfulness score too low: 0.0. Expected > 0.7\n", "trace": "  File \"C:\\Users\\SG0704235\\PycharmProjects\\ragas-openai-pytest\\utilities\\assertions.py\", line 15, in assert_faithfulness\n    assert score > threshold, f\"❌ Faithfulness score too low: {score}. Expected > {threshold}\"\n           ^^^^^^^^^^^^^^^^^\n"}, "start": 1762675123238, "stop": 1762675123238}], "attachments": [{"name": "log", "source": "da2483c3-b933-4c4c-915c-6e597742ed52-attachment.txt", "type": "text/plain"}, {"name": "stdout", "source": "211e1504-aafc-45e8-b41f-d13d4df220c1-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "get_singleturn_data", "value": "{'user_input': 'What should happen when adding seats or ancillaries to an existing order?', 'reference_contexts': ['Offers must be prepared for a specific order with a specific FF# and tier level.'], 'reference': 'When adding seats or ancillaries, offers must be generated specifically for the order and linked to the passenger’s FF# and tier level.', 'synthesizer_name': 'single_hop_specific_query_synthesizer'}"}], "start": 1762675110280, "stop": 1762675123239, "uuid": "5438339b-2d03-466a-a3cd-45149a0e6486", "historyId": "b553fa023f921f3c6e892665378859f2", "testCaseId": "290b6c32cedca0eae21329c6285d711f", "fullName": "tests.test_loyalty_tier_offers.TestLoyaltyTierOffers#test_faithfulness", "labels": [{"name": "severity", "value": "critical"}, {"name": "suite", "value": "Loyalty Tier Offers Evaluation Suite"}, {"name": "feature", "value": "Loyalty Tier Offers"}, {"name": "tag", "value": "asyncio"}, {"name": "parentSuite", "value": "tests"}, {"name": "subSuite", "value": "TestLoyaltyTierOffers"}, {"name": "host", "value": "W4KPW724"}, {"name": "thread", "value": "10228-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.test_loyalty_tier_offers"}], "titlePath": ["tests", "test_loyalty_tier_offers.py", "TestLoyaltyTierOffers"]}