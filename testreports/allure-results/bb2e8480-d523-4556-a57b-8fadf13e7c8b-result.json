{"name": "test_context_recall[get_singleturn_data2]", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ Context Recall too low: 0.0. Expected > 0.7", "trace": "self = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x000001ED577DA360>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='In a round trip for a single passenger with an FF#, what should the system do?', retrieve...ystem should allow adding seats and ancillaries at discounted prices according to the passenger’s tier.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\n\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        score = await evaluator.get_context_recall_score(get_singleturn_data)\n        logger.info(f\"Context Recall Score: {score}\")\n    \n>       Assertions.assert_context_recall(score)\n\ntest_loyalty_tier_offers.py:60: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nscore = 0.0, threshold = 0.7\n\n    @staticmethod\n    def assert_context_recall(score: float, threshold: float = 0.7):\n        \"\"\"\n        Validate that the Context Recall score meets the minimum threshold.\n        \"\"\"\n        with allure.step(f\"Validate Context Recall ≥ {threshold}\"):\n>           assert score > threshold, f\"❌ Context Recall too low: {score}. Expected > {threshold}\"\n                   ^^^^^^^^^^^^^^^^^\nE           AssertionError: ❌ Context Recall too low: 0.0. Expected > 0.7\n\n..\\utilities\\assertions.py:25: AssertionError"}, "description": "Validates that all relevant pieces of context were successfully retrieved (Context Recall).", "steps": [{"name": "Calculate Context Recall Score", "status": "passed", "attachments": [{"name": "User Input", "source": "d93d55a9-425e-4aa7-b39f-3ec6c8c2b676-attachment.txt", "type": "text/plain"}, {"name": "Retrieved Contexts", "source": "293c2a53-a719-4b82-98f8-986f83748ab6-attachment.txt", "type": "text/plain"}, {"name": "Reference Response", "source": "2fff1f9f-a06d-4f81-9fe4-57c62f997b0f-attachment.txt", "type": "text/plain"}, {"name": "Context Recall Score", "source": "915b72f4-f3f8-428f-9ded-011b53f2f644-attachment.txt", "type": "text/plain"}], "start": 1762763918313, "stop": 1762763925902}, {"name": "Context Recall Score: 0.0", "status": "passed", "start": 1762763925902, "stop": 1762763925902}, {"name": "Validate Context Recall ≥ 0.7", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ Context Recall too low: 0.0. Expected > 0.7\n", "trace": "  File \"C:\\Users\\SG0704235\\PycharmProjects\\ragas-openai-pytest\\utilities\\assertions.py\", line 25, in assert_context_recall\n    assert score > threshold, f\"❌ Context Recall too low: {score}. Expected > {threshold}\"\n           ^^^^^^^^^^^^^^^^^\n"}, "start": 1762763925902, "stop": 1762763925903}], "attachments": [{"name": "log", "source": "2904e342-4ca6-4e3c-b4ba-295230f74613-attachment.txt", "type": "text/plain"}, {"name": "stdout", "source": "bb46581b-8fa2-452c-8832-d9fe6e30dd8b-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "get_singleturn_data", "value": "{'user_input': 'In a round trip for a single passenger with an FF#, what should the system do?', 'reference_contexts': ['A paid air order with an FF# allows the user to add seats and ancillaries at a discounted price based on the passenger’s tier.'], 'reference': 'For a single passenger round trip with an FF#, the system should allow adding seats and ancillaries at discounted prices according to the passenger’s tier.', 'synthesizer_name': 'single_hop_specific_query_synthesizer'}"}], "start": 1762763918312, "stop": 1762763925904, "uuid": "3f3ae87e-7c45-4e78-992b-47229ec69796", "historyId": "3c5a024b8586950c473f2a5f405c8e9c", "testCaseId": "3620d216ffaf6b46cc45091a940a2104", "fullName": "tests.test_loyalty_tier_offers.TestLoyaltyTierOffers#test_context_recall", "labels": [{"name": "feature", "value": "Loyalty Tier Offers"}, {"name": "suite", "value": "Loyalty Tier Offers Evaluation Suite"}, {"name": "severity", "value": "minor"}, {"name": "tag", "value": "asyncio"}, {"name": "parentSuite", "value": "tests"}, {"name": "subSuite", "value": "TestLoyaltyTierOffers"}, {"name": "host", "value": "W4KPW724"}, {"name": "thread", "value": "30064-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.test_loyalty_tier_offers"}], "titlePath": ["tests", "test_loyalty_tier_offers.py", "TestLoyaltyTierOffers"]}