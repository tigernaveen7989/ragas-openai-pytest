{"name": "test_aspect_critic[get_multiturn_data2]", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ AspectCritic score too low: nan. Expected > 0.7. Aspect: nan", "trace": "self = <tests.test_rest_assured.TestRestAssured object at 0x0000019C9C63A3F0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...eliable, and maintainable automated tests that cover both the user interface and underlying APIs of modern web apps.'])\nlogger = <Logger pytest_logger (DEBUG)>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       self.assertions.assert_aspect_critic(result, threshold=0.7)\n\ntest_rest_assured.py:29: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000019C9C638740>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score ≥ {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"❌ AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: ❌ AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\n..\\utilities\\assertions.py:96: AssertionError"}, "description": "Validates that the model response remains aspect critic to the reference context for rest assured", "steps": [{"name": "Calculate AspectCritic (Aspect Critic:)", "status": "passed", "attachments": [{"name": "User Input", "source": "5ecb0122-1a40-4e10-8013-3fc61d616dcb-attachment.txt", "type": "text/plain"}, {"name": "AspectCritic Result", "source": "3961261e-40c0-4883-a8bd-0465c35aae4c-attachment.txt", "type": "text/plain"}], "start": 1763188374802, "stop": 1763188379400}, {"name": "AspectCritic Result: {'forgetfulness_aspect_critic': nan}", "status": "passed", "start": 1763188379400, "stop": 1763188379400}, {"name": "Validate AspectCritic Score ≥ 0.7", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n", "trace": "  File \"C:\\Users\\SG0704235\\PycharmProjects\\ragas-openai-pytest\\utilities\\assertions.py\", line 96, in assert_aspect_critic\n    assert score > threshold, (\n           ^^^^^^^^^^^^^^^^^\n"}, "start": 1763188379401, "stop": 1763188379401}], "attachments": [{"name": "log", "source": "0f345646-b3d3-4323-a949-b90c1185d7b5-attachment.txt", "type": "text/plain"}, {"name": "stdout", "source": "d8732c1e-b58f-4e45-b5ab-bb2da5ac700a-attachment.txt", "type": "text/plain"}, {"name": "stderr", "source": "ed989621-872c-40ed-9d32-48df639050bb-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "get_multiturn_data", "value": "{'conversation': [{'role': 'human', 'content': 'What is Playwright used for?'}, {'role': 'ai', 'content': 'Playwright is an end-to-end testing framework for web applications developed by Microsoft. It supports multiple browsers like Chromium, Firefox, and WebKit, and automates testing for UI and APIs.'}, {'role': 'human', 'content': 'What languages does Playwright support?'}, {'role': 'ai', 'content': 'Playwright supports JavaScript, TypeScript, Python, Java, and .NET, allowing flexibility for automation teams using different tech stacks.'}, {'role': 'human', 'content': 'Why is TypeScript preferred for Playwright projects?'}, {'role': 'ai', 'content': 'TypeScript adds static typing and better IntelliSense support, reducing runtime errors. It helps maintain large test codebases more efficiently compared to plain JavaScript.'}, {'role': 'human', 'content': 'How can I switch between JavaScript and TypeScript in Playwright?'}, {'role': 'ai', 'content': 'When creating a Playwright project, you can choose JavaScript or TypeScript mode using `npx playwright codegen` or `npx playwright init`. If you initially chose JavaScript, adding a `tsconfig.json` and renaming files to `.ts` will enable TypeScript support.'}, {'role': 'human', 'content': 'Can I use Playwright in CI/CD pipelines?'}, {'role': 'ai', 'content': 'Yes, Playwright integrates well with CI/CD tools like Jenkins, GitHub Actions, or GitLab CI. You can run headless tests in containers or virtual environments using `npx playwright test --reporter=html`.'}], 'reference_contexts': ['Playwright is a cross-browser testing tool supporting JavaScript and TypeScript.', 'TypeScript is preferred for better type safety and maintainability.', 'Playwright integrates easily with CI/CD pipelines for automated test execution.'], 'reference': 'Playwright is an end-to-end testing framework supporting JavaScript and TypeScript. TypeScript enhances reliability and scalability of test code. Playwright integrates with CI/CD tools for automated web testing.', 'synthesizer_name': 'multi_turn_playwright_typescript_javascript'}"}], "start": 1763188374801, "stop": 1763188379402, "uuid": "f26a1f61-fec1-4804-8e42-8d33aeceb3ef", "historyId": "ca465b38a12447fc5c1f7115ed1c1fac", "testCaseId": "e566cc050c8a2ea2f2d1829aee5ee020", "fullName": "tests.test_rest_assured.TestRestAssured#test_aspect_critic", "labels": [{"name": "suite", "value": "Rest Assured Evaluation Suite"}, {"name": "story", "value": "Aspect Critic Evaluation"}, {"name": "feature", "value": "Rest Assured"}, {"name": "tag", "value": "asyncio"}, {"name": "parentSuite", "value": "tests"}, {"name": "subSuite", "value": "TestRestAssured"}, {"name": "host", "value": "W4KPW724"}, {"name": "thread", "value": "26856-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.test_rest_assured"}], "titlePath": ["tests", "test_rest_assured.py", "TestRestAssured"]}