{"name": "test_aspect_critic[get_multiturn_data1]", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ AspectCritic score too low: nan. Expected > 0.7. Aspect: nan", "trace": "self = <tests.test_rest_assured.TestRestAssured object at 0x0000019C9C5D5A00>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ce usage to determine whether the system meets performance requirements and can scale to handle expected user loads.'])\nlogger = <Logger pytest_logger (DEBUG)>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       self.assertions.assert_aspect_critic(result, threshold=0.7)\n\ntest_rest_assured.py:29: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000019C9C638740>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score ≥ {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"❌ AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: ❌ AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\n..\\utilities\\assertions.py:96: AssertionError"}, "description": "Validates that the model response remains aspect critic to the reference context for rest assured", "steps": [{"name": "Calculate AspectCritic (Aspect Critic:)", "status": "passed", "attachments": [{"name": "User Input", "source": "89a7ec9a-c1d7-4573-b6ac-cf3ed9405b51-attachment.txt", "type": "text/plain"}, {"name": "AspectCritic Result", "source": "7e16eee5-7560-4ab7-85e4-bf0fe9db5d39-attachment.txt", "type": "text/plain"}], "start": 1763188368618, "stop": 1763188373325}, {"name": "AspectCritic Result: {'forgetfulness_aspect_critic': nan}", "status": "passed", "start": 1763188373325, "stop": 1763188373325}, {"name": "Validate AspectCritic Score ≥ 0.7", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n", "trace": "  File \"C:\\Users\\SG0704235\\PycharmProjects\\ragas-openai-pytest\\utilities\\assertions.py\", line 96, in assert_aspect_critic\n    assert score > threshold, (\n           ^^^^^^^^^^^^^^^^^\n"}, "start": 1763188373327, "stop": 1763188373327}], "attachments": [{"name": "log", "source": "30007370-5085-4f7e-a66f-f36ebef49e86-attachment.txt", "type": "text/plain"}, {"name": "stdout", "source": "ca28b67a-d1b0-4474-a6bf-a1eb22c35ddf-attachment.txt", "type": "text/plain"}, {"name": "stderr", "source": "752630e7-38d6-4294-baea-b9c6453770f2-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "get_multiturn_data", "value": "{'conversation': [{'role': 'human', 'content': 'What is performance testing?'}, {'role': 'ai', 'content': 'Performance testing is a type of non-functional testing that measures how an application performs under a specific workload. It checks parameters like speed, scalability, stability, and responsiveness.'}, {'role': 'human', 'content': 'What tools are commonly used for performance testing?'}, {'role': 'ai', 'content': 'Common tools include Apache JMeter, LoadRunner, Gatling, k6, and Locust. Among these, JMeter is widely used for open-source performance testing of web and API applications.'}, {'role': 'human', 'content': 'Can you explain how JMeter works?'}, {'role': 'ai', 'content': 'JMeter simulates multiple virtual users sending requests to a target server and measures how the system responds. It can test web applications, APIs, and databases by recording test plans, executing load scenarios, and collecting performance metrics like throughput, latency, and error rate.'}, {'role': 'human', 'content': 'How do you analyze JMeter test results?'}, {'role': 'ai', 'content': 'You can analyze JMeter results through listeners such as Summary Report, Aggregate Report, and Graph Results. Metrics like response time, error percentage, and throughput help identify performance bottlenecks and scalability issues.'}], 'reference_contexts': ['Performance testing evaluates the system’s responsiveness, stability, and scalability under load.', 'JMeter is an open-source tool that simulates concurrent users and measures performance metrics.', 'JMeter results can be analyzed through reports and graphs to find bottlenecks.'], 'reference': 'Performance testing ensures an application remains stable, scalable, and responsive under expected load. JMeter helps simulate concurrent user activity and analyze metrics such as throughput, latency, and error rate to identify performance issues.', 'synthesizer_name': 'multi_turn_performance_testing_jmeter'}"}], "start": 1763188368617, "stop": 1763188373328, "uuid": "fe1ff295-680f-40ae-b7ee-e4a2516b2e29", "historyId": "da514017cdd2f448a90c7df528c20e01", "testCaseId": "e566cc050c8a2ea2f2d1829aee5ee020", "fullName": "tests.test_rest_assured.TestRestAssured#test_aspect_critic", "labels": [{"name": "suite", "value": "Rest Assured Evaluation Suite"}, {"name": "story", "value": "Aspect Critic Evaluation"}, {"name": "feature", "value": "Rest Assured"}, {"name": "tag", "value": "asyncio"}, {"name": "parentSuite", "value": "tests"}, {"name": "subSuite", "value": "TestRestAssured"}, {"name": "host", "value": "W4KPW724"}, {"name": "thread", "value": "26856-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.test_rest_assured"}], "titlePath": ["tests", "test_rest_assured.py", "TestRestAssured"]}