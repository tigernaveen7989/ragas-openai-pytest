{"name": "test_rubric_score[get_singleturn_data1]", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ Rubrics too low: 2. Expected > 3", "trace": "self = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000026D83CD8F80>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens when a seat or ancillary is free for a passenger?', retrieved_contexts=['Whe...n a seat or ancillary is free, only an AE is generated with HK1/CONFIRMED status, and no EMD is issued.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\n\n    @allure.story(\"Rubrics Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubrics such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        score = await evaluator.get_rubric_score(get_singleturn_data)\n        logger.info(f\"Rubric Score: {score}\")\n    \n>       Assertions.assert_rubric(score)\n\ntest_loyalty_tier_offers.py:120: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nscore = 2, threshold = 3\n\n    @staticmethod\n    def assert_rubric(score: float, threshold: float = 3):\n        \"\"\"\n        Validate that the Rubrics score meets the minimum threshold.\n        \"\"\"\n        with allure.step(f\"Validate Rubrics Score ≥ {threshold}\"):\n>           assert score > threshold, f\"❌ Rubrics too low: {score}. Expected > {threshold}\"\n                   ^^^^^^^^^^^^^^^^^\nE           AssertionError: ❌ Rubrics too low: 2. Expected > 3\n\n..\\utilities\\assertions.py:81: AssertionError"}, "description": "Validates that the model response meets predefined quality rubrics such as accuracy, completeness, and coherence.", "steps": [{"name": "Calculate Rubric Score", "status": "passed", "attachments": [{"name": "User Input", "source": "855fc43c-915c-4167-81e4-4f8353c4ba25-attachment.txt", "type": "text/plain"}, {"name": "Retrieved Contexts", "source": "97c3f558-fd89-466d-a1f6-d1a02a1fa287-attachment.txt", "type": "text/plain"}, {"name": "Model Response", "source": "4bf7c413-579b-4dc5-8171-e6f06c47ddb2-attachment.txt", "type": "text/plain"}, {"name": "Reference Answer", "source": "6854f69a-de4a-44f1-85e9-47e734f806ed-attachment.txt", "type": "text/plain"}, {"name": "Rubric Definitions", "source": "68477b56-fd97-4f0a-8159-92fe82a5aa4e-attachment.json", "type": "application/json"}, {"name": "Rubric Score", "source": "a161869d-bf32-4b38-948b-071080215fce-attachment.txt", "type": "text/plain"}], "start": 1762765642066, "stop": 1762765648885}, {"name": "Rubric Score: 2", "status": "passed", "start": 1762765648885, "stop": 1762765648885}, {"name": "Validate Rubrics Score ≥ 3", "status": "failed", "statusDetails": {"message": "AssertionError: ❌ Rubrics too low: 2. Expected > 3\n", "trace": "  File \"C:\\Users\\SG0704235\\PycharmProjects\\ragas-openai-pytest\\utilities\\assertions.py\", line 81, in assert_rubric\n    assert score > threshold, f\"❌ Rubrics too low: {score}. Expected > {threshold}\"\n           ^^^^^^^^^^^^^^^^^\n"}, "start": 1762765648886, "stop": 1762765648886}], "attachments": [{"name": "log", "source": "8270892f-1da2-4bbd-9f20-0dd2bbc66eae-attachment.txt", "type": "text/plain"}, {"name": "stdout", "source": "b82c94f2-1725-4ba3-8265-edc5c9de0872-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "get_singleturn_data", "value": "{'user_input': 'What happens when a seat or ancillary is free for a passenger?', 'reference_contexts': ['If seats/ancillaries are free (0.00), only AE is generated with HK1/CONFIRMED status.'], 'reference': 'When a seat or ancillary is free, only an AE is generated with HK1/CONFIRMED status, and no EMD is issued.', 'synthesizer_name': 'single_hop_specific_query_synthesizer'}"}], "start": 1762765642066, "stop": 1762765648889, "uuid": "e43a55de-a414-49f1-aac9-de3444eff5d4", "historyId": "2ee5be3a1add57436ba6f683149d2939", "testCaseId": "89bacb780fc9314d27c602ec92699428", "fullName": "tests.test_loyalty_tier_offers.TestLoyaltyTierOffers#test_rubric_score", "labels": [{"name": "feature", "value": "Loyalty Tier Offers"}, {"name": "severity", "value": "critical"}, {"name": "suite", "value": "Loyalty Tier Offers Evaluation Suite"}, {"name": "story", "value": "Rubrics Evaluation"}, {"name": "tag", "value": "asyncio"}, {"name": "parentSuite", "value": "tests"}, {"name": "subSuite", "value": "TestLoyaltyTierOffers"}, {"name": "host", "value": "W4KPW724"}, {"name": "thread", "value": "19780-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.test_loyalty_tier_offers"}], "titlePath": ["tests", "test_loyalty_tier_offers.py", "TestLoyaltyTierOffers"]}