{
  "specs": {},
  "functional_spec_count": {},
  "testsuites": [
    {
      "cases": [
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x000002DDB8CEE660>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x000002DDB91510A0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntest_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\llm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n..\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x000002DDB8CEE7E0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ntent': 'How do you analyze JMeter test results?', 'role': 'human'}, ...], 'question': 'What is performance testing?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x000002DDB922FFB0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntest_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\llm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n..\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x000002DDB8CEE990>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x000002DDB91B3EC0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntest_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\llm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n..\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x000002DDB8CEEA20>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Jenkins used for?', metadata=None, type='human'), AIMessage... Jenkins run Playwright or API tests automatically?', 'role': 'human'}, ...], 'question': 'What is Jenkins used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x000002DDBC2DC0B0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntest_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\llm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n..\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n..\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x000002DDB8CEEAB0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is SabreMosaic and how does it help with canceling air exchang...istance.\", 'role': 'ai'}], 'question': 'What is SabreMosaic and how does it help with canceling air exchanged items?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x000002DDBC26B740>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntest_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\llm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n..\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n..\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x000002DDB8CEF7D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n..\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        }
      ]
    }
  ],
  "test_environment": "Development",
  "timestamp": "28 Nov 2025, 11:10",
  "img_url": "https://icon.icepanel.io/Technology/svg/pytest.svg",
  "test_status": "complete",
  "report_title": "pytest HTML Report",
  "category_count": {},
  "all_categories": []
}