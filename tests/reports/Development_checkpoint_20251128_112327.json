{
  "specs": {},
  "functional_spec_count": {},
  "testsuites": [
    {
      "cases": [
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c Conversational Memory too low: 0.57. Expected > 0.8\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000291475DF290>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x000002914784F0E0>\n\n    @allure.story(\"Coversational Memory Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains conversational memory to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_conversational_memory_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate conversational memory score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        score = await evaluator.get_conversational_memory_score(sample=sample, response=response, chat_history=question_chathistory.get(\"chat_history\", []))\n        logger.info(f\"Conversational Memory Score: {score}\")\n>       assertions.assert_conversational_memory(score=score, threshold=0.8)\n\ntest_rest_assured.py:77: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x000002914784F0E0>\nscore = 0.57, threshold = 0.8\n\n    def assert_conversational_memory(self, score: float, threshold: float = 0.8):\n        \"\"\"\n        Validate that the Conversational Memory score meets the minimum threshold.\n        \"\"\"\n        self.logger.info(f\"Expected Threshold: {threshold}, Score: {score}\")\n        with allure.step(f\"Validate Conversational Memory Score \u2265 {threshold}\"):\n>           assert score >= threshold, f\"\u274c Conversational Memory too low: {score}. Expected > {threshold}\"\n                   ^^^^^^^^^^^^^^^^^^\nE           AssertionError: \u274c Conversational Memory too low: 0.57. Expected > 0.8\n\n..\\utilities\\assertions.py:143: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Passed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c Conversational Memory too low: 0.525. Expected > 0.8\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000291475DF530>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000029147C80860>\n\n    @allure.story(\"Coversational Memory Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains conversational memory to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_conversational_memory_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate conversational memory score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        score = await evaluator.get_conversational_memory_score(sample=sample, response=response, chat_history=question_chathistory.get(\"chat_history\", []))\n        logger.info(f\"Conversational Memory Score: {score}\")\n>       assertions.assert_conversational_memory(score=score, threshold=0.8)\n\ntest_rest_assured.py:77: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000029147C80860>\nscore = 0.525, threshold = 0.8\n\n    def assert_conversational_memory(self, score: float, threshold: float = 0.8):\n        \"\"\"\n        Validate that the Conversational Memory score meets the minimum threshold.\n        \"\"\"\n        self.logger.info(f\"Expected Threshold: {threshold}, Score: {score}\")\n        with allure.step(f\"Validate Conversational Memory Score \u2265 {threshold}\"):\n>           assert score >= threshold, f\"\u274c Conversational Memory too low: {score}. Expected > {threshold}\"\n                   ^^^^^^^^^^^^^^^^^^\nE           AssertionError: \u274c Conversational Memory too low: 0.525. Expected > 0.8\n\n..\\utilities\\assertions.py:143: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Passed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c Conversational Memory too low: 0.2. Expected > 0.8\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000291475DF6B0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is SabreMosaic and how does it help with canceling air exchang...istance.\", 'role': 'ai'}], 'question': 'What is SabreMosaic and how does it help with canceling air exchanged items?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000029147C7C3B0>\n\n    @allure.story(\"Coversational Memory Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains conversational memory to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_conversational_memory_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate conversational memory score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        score = await evaluator.get_conversational_memory_score(sample=sample, response=response, chat_history=question_chathistory.get(\"chat_history\", []))\n        logger.info(f\"Conversational Memory Score: {score}\")\n>       assertions.assert_conversational_memory(score=score, threshold=0.8)\n\ntest_rest_assured.py:77: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000029147C7C3B0>\nscore = 0.2, threshold = 0.8\n\n    def assert_conversational_memory(self, score: float, threshold: float = 0.8):\n        \"\"\"\n        Validate that the Conversational Memory score meets the minimum threshold.\n        \"\"\"\n        self.logger.info(f\"Expected Threshold: {threshold}, Score: {score}\")\n        with allure.step(f\"Validate Conversational Memory Score \u2265 {threshold}\"):\n>           assert score >= threshold, f\"\u274c Conversational Memory too low: {score}. Expected > {threshold}\"\n                   ^^^^^^^^^^^^^^^^^^\nE           AssertionError: \u274c Conversational Memory too low: 0.2. Expected > 0.8\n\n..\\utilities\\assertions.py:143: AssertionError",
          "functional_specifications": [],
          "categories": []
        }
      ]
    }
  ],
  "test_environment": "Development",
  "timestamp": "28 Nov 2025, 11:24",
  "img_url": "https://icon.icepanel.io/Technology/svg/pytest.svg",
  "test_status": "complete",
  "report_title": "pytest HTML Report",
  "category_count": {},
  "all_categories": []
}