{
  "specs": {},
  "functional_spec_count": {},
  "testsuites": [
    {
      "cases": [
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x0000025140893290>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000025140D6C110>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000025140D6C110>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000251408F9CA0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ntent': 'How do you analyze JMeter test results?', 'role': 'human'}, ...], 'question': 'What is performance testing?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000025140E86DB0>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000025140E86DB0>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000251408F9DF0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000251411204D0>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x00000251411204D0>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000251408F9E80>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Jenkins used for?', metadata=None, type='human'), AIMessage... Jenkins run Playwright or API tests automatically?', 'role': 'human'}, ...], 'question': 'What is Jenkins used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000251411748C0>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x00000251411748C0>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000251408F9F10>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is SabreMosaic and how does it help with canceling air exchang...istance.\", 'role': 'ai'}], 'question': 'What is SabreMosaic and how does it help with canceling air exchanged items?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000025141120290>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000025141120290>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        }
      ]
    }
  ],
  "test_environment": "Development",
  "timestamp": "24 Nov 2025, 16:06",
  "img_url": "https://icon.icepanel.io/Technology/svg/pytest.svg",
  "test_status": "complete",
  "report_title": "pytest HTML Report",
  "category_count": {},
  "all_categories": []
}