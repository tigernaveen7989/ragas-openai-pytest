{
  "specs": {},
  "functional_spec_count": {},
  "testsuites": [
    {
      "cases": [
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FE28080>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What is the purpose of the Seat Map & Ancillary Offers feature with Frequent Flyer Tier B...t flyer tier benefits, making certain items free or discounted according to the passenger's tier level.\", rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D003CF680>\n\n    @allure.story(\"Faithfulness Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response remains faithful to the reference context for loyalty tier offers.\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_faithfulness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate faithfulness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_faithfulness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:24: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:52: in get_faithfulness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:200: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:208: in _ascore\n    statements = await self._create_statements(row, callbacks)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:174: in _create_statements\n    statements = await self.statement_generator_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA6270>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens when a seat or ancillary is free for a passenger?', retrieved_contexts=['###...n a seat or ancillary is free, only an AE is generated with HK1/CONFIRMED status, and no EMD is issued.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00597680>\n\n    @allure.story(\"Faithfulness Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response remains faithful to the reference context for loyalty tier offers.\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_faithfulness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate faithfulness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_faithfulness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:24: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:52: in get_faithfulness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:200: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:208: in _ascore\n    statements = await self._create_statements(row, callbacks)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:174: in _create_statements\n    statements = await self.statement_generator_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA65D0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='How are tier benefits applied when two passengers have different frequent flyer tiers?', ...ording to their own tier level. Some shared benefits, like lounge access, may depend on airline policy.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D009A2B10>\n\n    @allure.story(\"Faithfulness Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response remains faithful to the reference context for loyalty tier offers.\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_faithfulness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate faithfulness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_faithfulness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:24: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:52: in get_faithfulness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:200: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:208: in _ascore\n    statements = await self._create_statements(row, callbacks)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:174: in _create_statements\n    statements = await self.statement_generator_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data5]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA6750>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What validation mechanism ensures the accuracy of seat or ancillary offers during modific...nd timestamp validation in OMS ensures accuracy and consistency during seat or ancillary modifications.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04644EC0>\n\n    @allure.story(\"Faithfulness Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response remains faithful to the reference context for loyalty tier offers.\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_faithfulness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate faithfulness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_faithfulness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:24: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:52: in get_faithfulness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:200: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:208: in _ascore\n    statements = await self._create_statements(row, callbacks)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:174: in _create_statements\n    statements = await self.statement_generator_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data6]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_faithfulness[get_singleturn_data7]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA6900>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens if seats or ancillaries are chargeable but discounted?', retrieved_contexts=...of extra services and boost revenue, while passengers benefit from reduced prices and flexible options.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D0470D0A0>\n\n    @allure.story(\"Faithfulness Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response remains faithful to the reference context for loyalty tier offers.\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_faithfulness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate faithfulness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_faithfulness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:24: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:52: in get_faithfulness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:200: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:208: in _ascore\n    statements = await self._create_statements(row, callbacks)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py:174: in _create_statements\n    statements = await self.statement_generator_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA6DE0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What is the purpose of the Seat Map & Ancillary Offers feature with Frequent Flyer Tier B...t flyer tier benefits, making certain items free or discounted according to the passenger's tier level.\", rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D0471C320>\n\n    @allure.story(\"Context Precision Evaluation\")\n    @allure.severity(allure.severity_level.NORMAL)\n    @allure.description(\n        \"Validates that retrieved contexts are relevant to the user query (Context Precision).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_precision(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Precision score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_precision_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:44: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:81: in get_context_precision_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:318: in _single_turn_ascore\n    return await super()._single_turn_ascore(sample, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:324: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:152: in _ascore\n    ] = await self.context_precision_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7200>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='In a round trip for a single passenger with an FF#, what should the system do?', retrieve...ystem should allow adding seats and ancillaries at discounted prices according to the passenger\u2019s tier.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D009A3F20>\n\n    @allure.story(\"Context Precision Evaluation\")\n    @allure.severity(allure.severity_level.NORMAL)\n    @allure.description(\n        \"Validates that retrieved contexts are relevant to the user query (Context Precision).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_precision(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Precision score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_precision_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:44: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:81: in get_context_precision_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:318: in _single_turn_ascore\n    return await super()._single_turn_ascore(sample, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:324: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:152: in _ascore\n    ] = await self.context_precision_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA72C0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='How are tier benefits applied when two passengers have different frequent flyer tiers?', ...ording to their own tier level. Some shared benefits, like lounge access, may depend on airline policy.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D046442F0>\n\n    @allure.story(\"Context Precision Evaluation\")\n    @allure.severity(allure.severity_level.NORMAL)\n    @allure.description(\n        \"Validates that retrieved contexts are relevant to the user query (Context Precision).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_precision(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Precision score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_precision_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:44: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:81: in get_context_precision_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:318: in _single_turn_ascore\n    return await super()._single_turn_ascore(sample, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:324: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:152: in _ascore\n    ] = await self.context_precision_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7380>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens in a two-passenger one-way scenario with FF numbers?', retrieved_contexts=[\"...o, both passengers with FF numbers receive discounted offers, and EMD and AE are generated accordingly.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D005F6F90>\n\n    @allure.story(\"Context Precision Evaluation\")\n    @allure.severity(allure.severity_level.NORMAL)\n    @allure.description(\n        \"Validates that retrieved contexts are relevant to the user query (Context Precision).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_precision(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Precision score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_precision_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:44: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:81: in get_context_precision_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:318: in _single_turn_ascore\n    return await super()._single_turn_ascore(sample, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:324: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:152: in _ascore\n    ] = await self.context_precision_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data5]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7440>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What validation mechanism ensures the accuracy of seat or ancillary offers during modific...nd timestamp validation in OMS ensures accuracy and consistency during seat or ancillary modifications.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D0471E060>\n\n    @allure.story(\"Context Precision Evaluation\")\n    @allure.severity(allure.severity_level.NORMAL)\n    @allure.description(\n        \"Validates that retrieved contexts are relevant to the user query (Context Precision).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_precision(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Precision score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_precision_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:44: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:81: in get_context_precision_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:318: in _single_turn_ascore\n    return await super()._single_turn_ascore(sample, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:324: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:152: in _ascore\n    ] = await self.context_precision_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data6]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_precision[get_singleturn_data7]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA75F0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens if seats or ancillaries are chargeable but discounted?', retrieved_contexts=...of extra services and boost revenue, while passengers benefit from reduced prices and flexible options.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04689820>\n\n    @allure.story(\"Context Precision Evaluation\")\n    @allure.severity(allure.severity_level.NORMAL)\n    @allure.description(\n        \"Validates that retrieved contexts are relevant to the user query (Context Precision).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_precision(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Precision score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_precision_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:44: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:81: in get_context_precision_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:318: in _single_turn_ascore\n    return await super()._single_turn_ascore(sample, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:138: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:324: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py:152: in _ascore\n    ] = await self.context_precision_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7CE0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What is the purpose of the Seat Map & Ancillary Offers feature with Frequent Flyer Tier B...t flyer tier benefits, making certain items free or discounted according to the passenger's tier level.\", rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04647230>\n\n    @allure.story(\"Context Recall Evaluation\")\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_recall_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:104: in get_context_recall_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:173: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:138: in _ascore\n    ] = await self.context_recall_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7F80>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens when a seat or ancillary is free for a passenger?', retrieved_contexts=['Sho...n a seat or ancillary is free, only an AE is generated with HK1/CONFIRMED status, and no EMD is issued.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D047027E0>\n\n    @allure.story(\"Context Recall Evaluation\")\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_recall_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:104: in get_context_recall_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:173: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:138: in _ascore\n    ] = await self.context_recall_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7680>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='In a round trip for a single passenger with an FF#, what should the system do?', retrieve...ystem should allow adding seats and ancillaries at discounted prices according to the passenger\u2019s tier.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D0471EA80>\n\n    @allure.story(\"Context Recall Evaluation\")\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_recall_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:104: in get_context_recall_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:173: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:138: in _ascore\n    ] = await self.context_recall_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA73B0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens in a two-passenger one-way scenario with FF numbers?', retrieved_contexts=[\"...o, both passengers with FF numbers receive discounted offers, and EMD and AE are generated accordingly.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00F38170>\n\n    @allure.story(\"Context Recall Evaluation\")\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_recall_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:104: in get_context_recall_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:173: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:138: in _ascore\n    ] = await self.context_recall_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data5]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA71D0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What validation mechanism ensures the accuracy of seat or ancillary offers during modific...nd timestamp validation in OMS ensures accuracy and consistency during seat or ancillary modifications.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00F3A6F0>\n\n    @allure.story(\"Context Recall Evaluation\")\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_recall_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:104: in get_context_recall_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:173: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:138: in _ascore\n    ] = await self.context_recall_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data6]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA6120>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What should happen when adding seats or ancillaries to an existing order?', retrieved_con..., offers must be generated specifically for the order and linked to the passenger\u2019s FF# and tier level.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00EC7B60>\n\n    @allure.story(\"Context Recall Evaluation\")\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_recall_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:104: in get_context_recall_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:173: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:138: in _ascore\n    ] = await self.context_recall_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_context_recall[get_singleturn_data7]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA6810>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens if seats or ancillaries are chargeable but discounted?', retrieved_contexts=...of extra services and boost revenue, while passengers benefit from reduced prices and flexible options.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04687050>\n\n    @allure.story(\"Context Recall Evaluation\")\n    @allure.severity(allure.severity_level.MINOR)\n    @allure.description(\n        \"Validates that all relevant pieces of context were successfully retrieved (Context Recall).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_context_recall(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Context Recall score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_context_recall_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:62: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:104: in get_context_recall_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:169: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:173: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py:138: in _ascore\n    ] = await self.context_recall_prompt.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF30590>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What is the purpose of the Seat Map & Ancillary Offers feature with Frequent Flyer Tier B...t flyer tier benefits, making certain items free or discounted according to the passenger's tier level.\", rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04686CF0>\n\n    @allure.story(\"Answer Relavancy Evaluation\")\n    @allure.severity(allure.severity_level.BLOCKER)\n    @allure.description(\n        \"Validates that the model response is relevant to the user query (Answer Relevancy).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_answer_relevancy(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Answer Relevancy score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_answer_relevancy_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:80: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:127: in get_answer_relevancy_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:135: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:151: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:142: in _ascore\n    responses = await self.question_generation.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF30860>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens when a seat or ancillary is free for a passenger?', retrieved_contexts=['In ...n a seat or ancillary is free, only an AE is generated with HK1/CONFIRMED status, and no EMD is issued.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D0464E5D0>\n\n    @allure.story(\"Answer Relavancy Evaluation\")\n    @allure.severity(allure.severity_level.BLOCKER)\n    @allure.description(\n        \"Validates that the model response is relevant to the user query (Answer Relevancy).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_answer_relevancy(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Answer Relevancy score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_answer_relevancy_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:80: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:127: in get_answer_relevancy_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:135: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:151: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:142: in _ascore\n    responses = await self.question_generation.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF30950>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='In a round trip for a single passenger with an FF#, what should the system do?', retrieve...ystem should allow adding seats and ancillaries at discounted prices according to the passenger\u2019s tier.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D009A3DD0>\n\n    @allure.story(\"Answer Relavancy Evaluation\")\n    @allure.severity(allure.severity_level.BLOCKER)\n    @allure.description(\n        \"Validates that the model response is relevant to the user query (Answer Relevancy).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_answer_relevancy(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Answer Relevancy score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_answer_relevancy_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:80: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:127: in get_answer_relevancy_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:135: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:151: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:142: in _ascore\n    responses = await self.question_generation.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF30B30>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens in a two-passenger one-way scenario with FF numbers?', retrieved_contexts=['...o, both passengers with FF numbers receive discounted offers, and EMD and AE are generated accordingly.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00F108F0>\n\n    @allure.story(\"Answer Relavancy Evaluation\")\n    @allure.severity(allure.severity_level.BLOCKER)\n    @allure.description(\n        \"Validates that the model response is relevant to the user query (Answer Relevancy).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_answer_relevancy(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Answer Relevancy score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_answer_relevancy_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:80: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:127: in get_answer_relevancy_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:135: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:151: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:142: in _ascore\n    responses = await self.question_generation.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data5]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF30C20>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What validation mechanism ensures the accuracy of seat or ancillary offers during modific...nd timestamp validation in OMS ensures accuracy and consistency during seat or ancillary modifications.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00F13E30>\n\n    @allure.story(\"Answer Relavancy Evaluation\")\n    @allure.severity(allure.severity_level.BLOCKER)\n    @allure.description(\n        \"Validates that the model response is relevant to the user query (Answer Relevancy).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_answer_relevancy(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Answer Relevancy score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_answer_relevancy_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:80: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:127: in get_answer_relevancy_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:135: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:151: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:142: in _ascore\n    responses = await self.question_generation.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data6]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF30D10>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What should happen when adding seats or ancillaries to an existing order?', retrieved_con..., offers must be generated specifically for the order and linked to the passenger\u2019s FF# and tier level.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00E741A0>\n\n    @allure.story(\"Answer Relavancy Evaluation\")\n    @allure.severity(allure.severity_level.BLOCKER)\n    @allure.description(\n        \"Validates that the model response is relevant to the user query (Answer Relevancy).\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_answer_relevancy(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Answer Relevancy score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_answer_relevancy_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:80: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:127: in get_answer_relevancy_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:135: in _single_turn_ascore\n    return await self._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:151: in _ascore\n    return await super()._ascore(row, callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py:142: in _ascore\n    responses = await self.question_generation.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...n}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 3, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_answer_relevancy[get_singleturn_data7]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF314F0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What is the purpose of the Seat Map & Ancillary Offers feature with Frequent Flyer Tier B...t flyer tier benefits, making certain items free or discounted according to the passenger's tier level.\", rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00EB8830>\n\n    @allure.story(\"Factual Correctness Evaluation\")\n    @allure.severity(allure.severity_level.TRIVIAL)\n    @allure.description(\n        \"Validates that the generated answer is factually correct with respect to the reference answer.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_factual_correctness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Factual Correctness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_factual_correctness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:98: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:150: in get_factual_correctness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:278: in _single_turn_ascore\n    reference_response, response_reference = await asyncio.gather(\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:301: in decompose_and_verify_claims\n    claims = await self.decompose_claims(response, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:231: in decompose_claims\n    result = await self.claim_decomposition_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31790>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens when a seat or ancillary is free for a passenger?', retrieved_contexts=['Whe...n a seat or ancillary is free, only an AE is generated with HK1/CONFIRMED status, and no EMD is issued.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00EBB590>\n\n    @allure.story(\"Factual Correctness Evaluation\")\n    @allure.severity(allure.severity_level.TRIVIAL)\n    @allure.description(\n        \"Validates that the generated answer is factually correct with respect to the reference answer.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_factual_correctness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Factual Correctness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_factual_correctness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:98: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:150: in get_factual_correctness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:278: in _single_turn_ascore\n    reference_response, response_reference = await asyncio.gather(\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:301: in decompose_and_verify_claims\n    claims = await self.decompose_claims(response, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:231: in decompose_claims\n    result = await self.claim_decomposition_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31880>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='In a round trip for a single passenger with an FF#, what should the system do?', retrieve...ystem should allow adding seats and ancillaries at discounted prices according to the passenger\u2019s tier.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00E6FFB0>\n\n    @allure.story(\"Factual Correctness Evaluation\")\n    @allure.severity(allure.severity_level.TRIVIAL)\n    @allure.description(\n        \"Validates that the generated answer is factually correct with respect to the reference answer.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_factual_correctness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Factual Correctness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_factual_correctness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:98: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:150: in get_factual_correctness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:278: in _single_turn_ascore\n    reference_response, response_reference = await asyncio.gather(\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:301: in decompose_and_verify_claims\n    claims = await self.decompose_claims(response, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:231: in decompose_claims\n    result = await self.claim_decomposition_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31970>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='How are tier benefits applied when two passengers have different frequent flyer tiers?', ...ording to their own tier level. Some shared benefits, like lounge access, may depend on airline policy.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00E6ECF0>\n\n    @allure.story(\"Factual Correctness Evaluation\")\n    @allure.severity(allure.severity_level.TRIVIAL)\n    @allure.description(\n        \"Validates that the generated answer is factually correct with respect to the reference answer.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_factual_correctness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Factual Correctness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_factual_correctness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:98: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:150: in get_factual_correctness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:278: in _single_turn_ascore\n    reference_response, response_reference = await asyncio.gather(\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:301: in decompose_and_verify_claims\n    claims = await self.decompose_claims(response, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:231: in decompose_claims\n    result = await self.claim_decomposition_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data5]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31B50>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What validation mechanism ensures the accuracy of seat or ancillary offers during modific...nd timestamp validation in OMS ensures accuracy and consistency during seat or ancillary modifications.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D046865D0>\n\n    @allure.story(\"Factual Correctness Evaluation\")\n    @allure.severity(allure.severity_level.TRIVIAL)\n    @allure.description(\n        \"Validates that the generated answer is factually correct with respect to the reference answer.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_factual_correctness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Factual Correctness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_factual_correctness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:98: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:150: in get_factual_correctness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:278: in _single_turn_ascore\n    reference_response, response_reference = await asyncio.gather(\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:301: in decompose_and_verify_claims\n    claims = await self.decompose_claims(response, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:231: in decompose_claims\n    result = await self.claim_decomposition_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data6]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31C40>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What should happen when adding seats or ancillaries to an existing order?', retrieved_con..., offers must be generated specifically for the order and linked to the passenger\u2019s FF# and tier level.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00E6D220>\n\n    @allure.story(\"Factual Correctness Evaluation\")\n    @allure.severity(allure.severity_level.TRIVIAL)\n    @allure.description(\n        \"Validates that the generated answer is factually correct with respect to the reference answer.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_singleturn_data\", IronMan.load_test_data(feature_name), indirect=True)\n    async def test_factual_correctness(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Factual Correctness score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_factual_correctness_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:98: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:150: in get_factual_correctness_score\n    score = await self.metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:278: in _single_turn_ascore\n    reference_response, response_reference = await asyncio.gather(\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:301: in decompose_and_verify_claims\n    claims = await self.decompose_claims(response, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_factual_correctness.py:231: in decompose_claims\n    result = await self.claim_decomposition_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_factual_correctness[get_singleturn_data7]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7D10>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What is the purpose of the Seat Map & Ancillary Offers feature with Frequent Flyer Tier B...t flyer tier benefits, making certain items free or discounted according to the passenger's tier level.\", rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04680530>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubric such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:193: in get_rubric_score\n    score = await metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:135: in _single_turn_ascore\n    return await self._ascore(sample.to_dict(), callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:154: in _ascore\n    output = await self.single_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FEA7410>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens when a seat or ancillary is free for a passenger?', retrieved_contexts=[\"###...n a seat or ancillary is free, only an AE is generated with HK1/CONFIRMED status, and no EMD is issued.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00E6E930>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubric such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:193: in get_rubric_score\n    score = await metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:135: in _single_turn_ascore\n    return await self._ascore(sample.to_dict(), callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:154: in _ascore\n    output = await self.single_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31DF0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='In a round trip for a single passenger with an FF#, what should the system do?', retrieve...ystem should allow adding seats and ancillaries at discounted prices according to the passenger\u2019s tier.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00E74050>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubric such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:193: in get_rubric_score\n    score = await metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:135: in _single_turn_ascore\n    return await self._ascore(sample.to_dict(), callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:154: in _ascore\n    output = await self.single_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31BE0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='How are tier benefits applied when two passengers have different frequent flyer tiers?', ...ording to their own tier level. Some shared benefits, like lounge access, may depend on airline policy.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D050A6900>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubric such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:193: in get_rubric_score\n    score = await metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:135: in _single_turn_ascore\n    return await self._ascore(sample.to_dict(), callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:154: in _ascore\n    output = await self.single_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Skipped",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Type: Skip\nMessage: Skipped: \u274c Skipping test due to API internal server error.",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data5]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31940>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What validation mechanism ensures the accuracy of seat or ancillary offers during modific...nd timestamp validation in OMS ensures accuracy and consistency during seat or ancillary modifications.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D009A1FA0>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubric such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:193: in get_rubric_score\n    score = await metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:135: in _single_turn_ascore\n    return await self._ascore(sample.to_dict(), callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:154: in _ascore\n    output = await self.single_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data6]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31580>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What should happen when adding seats or ancillaries to an existing order?', retrieved_con..., offers must be generated specifically for the order and linked to the passenger\u2019s FF# and tier level.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04682FC0>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubric such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:193: in get_rubric_score\n    score = await metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:135: in _single_turn_ascore\n    return await self._ascore(sample.to_dict(), callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:154: in _ascore\n    output = await self.single_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_loyalty_tier_offers",
          "name": "test_rubric_score[get_singleturn_data7]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_loyalty_tier_offers.TestLoyaltyTierOffers object at 0x0000019D7FF31370>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_singleturn_data = SingleTurnSample(user_input='What happens if seats or ancillaries are chargeable but discounted?', retrieved_contexts=...of extra services and boost revenue, while passengers benefit from reduced prices and flexible options.', rubrics=None)\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D046868A0>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.severity(allure.severity_level.CRITICAL)\n    @allure.description(\n        \"Validates that the model response meets predefined quality rubric such as accuracy, completeness, and coherence.\"\n    )\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\n        \"get_singleturn_data\",\n        IronMan.load_test_data(\"loyalty_tier_offers\"),\n        indirect=True\n    )\n    async def test_rubric_score(self, get_llm_wrapper, get_singleturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(get_singleturn_data)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_loyalty_tier_offers.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:193: in get_rubric_score\n    score = await metric.single_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:558: in single_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:551: in single_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:135: in _single_turn_ascore\n    return await self._ascore(sample.to_dict(), callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:154: in _ascore\n    output = await self.single_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF50F80>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D046471D0>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000019D046471D0>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF51070>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ntent': 'How do you analyze JMeter test results?', 'role': 'human'}, ...], 'question': 'What is performance testing?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04C1E990>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000019D04C1E990>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF51220>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04B27890>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000019D04B27890>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF512E0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Jenkins used for?', metadata=None, type='human'), AIMessage... Jenkins run Playwright or API tests automatically?', 'role': 'human'}, ...], 'question': 'What is Jenkins used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04C1DC10>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000019D04C1DC10>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF513A0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is SabreMosaic and how does it help with canceling air exchang...istance.\", 'role': 'ai'}], 'question': 'What is SabreMosaic and how does it help with canceling air exchanged items?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04C1E9F0>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x0000019D04C1E9F0>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF332F0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04C1D7F0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF319A0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ntent': 'How do you analyze JMeter test results?', 'role': 'human'}, ...], 'question': 'What is performance testing?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D04C19B20>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF31B20>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00E6FC80>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FF31D60>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Jenkins used for?', metadata=None, type='human'), AIMessage... Jenkins run Playwright or API tests automatically?', 'role': 'human'}, ...], 'question': 'What is Jenkins used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00F11310>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x0000019D7FEA7350>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is SabreMosaic and how does it help with canceling air exchang...istance.\", 'role': 'ai'}], 'question': 'What is SabreMosaic and how does it help with canceling air exchanged items?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x0000019D00F2CEC0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x0000019D7FF51A00>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_rubric_score[get_multiturn_data0]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_rubric_score[get_multiturn_data1]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_rubric_score[get_multiturn_data2]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_rubric_score[get_multiturn_data3]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_rubric_score[get_multiturn_data4]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data0]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data1]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data2]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data3]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x0000019D455C9AF0>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data4]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        }
      ]
    }
  ],
  "test_environment": "Development",
  "timestamp": "28 Nov 2025, 13:27",
  "img_url": "https://icon.icepanel.io/Technology/svg/pytest.svg",
  "test_status": "complete",
  "report_title": "pytest HTML Report",
  "category_count": {},
  "all_categories": []
}