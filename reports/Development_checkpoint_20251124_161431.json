{
  "specs": {},
  "functional_spec_count": {},
  "testsuites": [
    {
      "cases": [
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000198C87FA180>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C8F8F5C0>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x00000198C8F8F5C0>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8B7B770>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ntent': 'How do you analyze JMeter test results?', 'role': 'human'}, ...], 'question': 'What is performance testing?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C917F830>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x00000198C917F830>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCCD40>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C93F9460>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x00000198C93F9460>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCD8E0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Jenkins used for?', metadata=None, type='human'), AIMessage... Jenkins run Playwright or API tests automatically?', 'role': 'human'}, ...], 'question': 'What is Jenkins used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C94404D0>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x00000198C94404D0>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_aspect_critic[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\nDetails: self = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCD970>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is SabreMosaic and how does it help with canceling air exchang...istance.\", 'role': 'ai'}], 'question': 'What is SabreMosaic and how does it help with canceling air exchanged items?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C93F9460>\n\n    @allure.story(\"Aspect Critic Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains aspect critic to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_aspect_critic(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        logger.info(get_multiturn_data)\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n        result = await evaluator.get_aspect_critic(sample=sample)\n        logger.info(f\"Aspect Critic Result: {result}\")\n>       assertions.assert_aspect_critic(result, threshold=0.7)\n\ntests\\test_rest_assured.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <utilities.assertions.Assertions object at 0x00000198C93F9460>\nresult = {'forgetfulness_aspect_critic': nan}, threshold = 0.7\n\n    def assert_aspect_critic(self, result, threshold: float = 0.7):\n        \"\"\"\n        Validate that the AspectCritic score meets the minimum threshold.\n        Result comes as {'aspectname_aspect_critic': score}.\n        \"\"\"\n        score_list = result.scores\n        score = score_list[0] if score_list else None\n        score = score['forgetfulness_aspect_critic']\n    \n        if score is None:\n            raise ValueError(\"No score found in EvaluationResult\")\n    \n        with allure.step(f\"Validate AspectCritic Score \u2265 {threshold}\"):\n>           assert score > threshold, (\n                   ^^^^^^^^^^^^^^^^^\n                f\"\u274c AspectCritic score too low: {score}. \"\n                f\"Expected > {threshold}. Aspect: {score}\"\n            )\nE           AssertionError: \u274c AspectCritic score too low: nan. Expected > 0.7. Aspect: nan\n\nutilities\\assertions.py:107: AssertionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCDBE0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C94403B0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCDD60>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ntent': 'How do you analyze JMeter test results?', 'role': 'human'}, ...], 'question': 'What is performance testing?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C917C620>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCDF10>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C94779E0>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCDFA0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Jenkins used for?', metadata=None, type='human'), AIMessage... Jenkins run Playwright or API tests automatically?', 'role': 'human'}, ...], 'question': 'What is Jenkins used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C947FB00>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_top_adherence_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCE030>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is SabreMosaic and how does it help with canceling air exchang...istance.\", 'role': 'ai'}], 'question': 'What is SabreMosaic and how does it help with canceling air exchanged items?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C950AF90>\n\n    @allure.story(\"Top Adherence Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains top adherence to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_top_adherence_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate aspect critic score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_top_adherence_score(sample=sample, response=response)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:317: in get_top_adherence_score\n    llm_response = await llm_client.agenerate_text(prompt_value)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...           ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCE2A0>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Rest Assured?', metadata=None, type='human'), AIMessage(con...egrate Rest Assured with a BDD framework like Cucumber?', 'role': 'human'}, ...], 'question': 'What is Rest Assured?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C94D37A0>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains rubric to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_rubric_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(sample)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:195: in get_rubric_score\n    score = await metric.multi_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:682: in multi_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:675: in multi_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:171: in _multi_turn_ascore\n    output = await self.multi_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCE420>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is performance testing?', metadata=None, type='human'), AIMess...ntent': 'How do you analyze JMeter test results?', 'role': 'human'}, ...], 'question': 'What is performance testing?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C94D0C20>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains rubric to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_rubric_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(sample)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:195: in get_rubric_score\n    score = await metric.multi_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:682: in multi_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:675: in multi_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:171: in _multi_turn_ascore\n    output = await self.multi_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Message: openai.APIConnectionError: Connection error.\nDetails: @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101: in handle_async_request\n    raise exc\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78: in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156: in _connect\n    stream = await stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67: in start_tls\n    with map_exceptions(exc_map):\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmap = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.Conn... <class 'anyio.EndOfStream'>: <class 'httpcore.ConnectError'>, <class 'ssl.SSLError'>: <class 'httpcore.ConnectError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.ConnectError\n\n.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n>               response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1529: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\httpx\\_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv\\Lib\\site-packages\\httpx\\_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393: in handle_async_request\n    with map_httpcore_exceptions():\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: in __exit__\n    self.gen.throw(value)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.ConnectError\n\n.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118: ConnectError\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.test_rest_assured.TestRestAssured object at 0x00000198C8BCE600>\nget_llm_wrapper = LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\nget_multiturn_data = (MultiTurnSample(user_input=[HumanMessage(content='What is Playwright used for?', metadata=None, type='human'), AIMess...between JavaScript and TypeScript in Playwright?', 'role': 'human'}, ...], 'question': 'What is Playwright used for?'})\nlogger = <Logger pytest_logger (DEBUG)>\nassertions = <utilities.assertions.Assertions object at 0x00000198C93FBA40>\n\n    @allure.story(\"Rubric Evaluation\")\n    @allure.description(\n        \"Validates that the model response remains rubric to the reference context for rest assured\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"get_multiturn_data\", IronMan.load_test_data(feature_name, \"multiturn\"), indirect=True)\n    async def test_rubric_score(self, get_llm_wrapper, get_multiturn_data, logger, assertions):\n        \"\"\"\n        Test to validate Rubric Score using reusable helper class.\n        \"\"\"\n        sample, response, question_chathistory = get_multiturn_data\n        evaluator = MetricsEvaluator(get_llm_wrapper)\n>       score = await evaluator.get_rubric_score(sample)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_rest_assured.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nllm_base\\ragas_metrics_evaluator.py:195: in get_rubric_score\n    score = await metric.multi_turn_ascore(sample)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:682: in multi_turn_ascore\n    raise e\n.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:675: in multi_turn_ascore\n    score = await asyncio.wait_for(\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:520: in wait_for\n    return await fut\n           ^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\metrics\\_domain_specific_rubrics.py:171: in _multi_turn_ascore\n    output = await self.multi_turn_scoring_prompt.generate(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:164: in generate\n    output_single = await self.generate_multiple(\n.venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:242: in generate_multiple\n    resp = await ragas_llm.generate(\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:117: in generate\n    result = await agenerate_text_with_retry(\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189: in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111: in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153: in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\_utils.py:99: in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\tenacity\\__init__.py:400: in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401: in __get_result\n    raise self._exception\n.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114: in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\ragas\\llms\\base.py:286: in agenerate_text\n    result = await self.langchain_llm.agenerate_prompt(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1099: in agenerate_prompt\n    return await self.agenerate(\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1057: in agenerate\n    raise exceptions[0]\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314: in __step_run_and_handle_result\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1310: in _agenerate_with_cache\n    result = await self._agenerate(\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1459: in _agenerate\n    raise e\n.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1452: in _agenerate\n    raw_response = await self.async_client.with_raw_response.create(\n.venv\\Lib\\site-packages\\openai\\_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585: in create\n    return await self._post(\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1794: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.AsyncOpenAI object at 0x00000198C8BCE5D0>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Raw-Response': 'true'}, m...}\\nOutput: ', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': False, 'temperature': 0.01}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n>               raise APIConnectionError(request=request) from err\nE               openai.APIConnectionError: Connection error.\n\n.venv\\Lib\\site-packages\\openai\\_base_client.py:1561: APIConnectionError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x000001988E560D10>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_rubric_score[get_multiturn_data3]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_rubric_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x000001988E560D10>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_rubric_score[get_multiturn_data4]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data0]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x000001988E560D10>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data0]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data1]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x000001988E560D10>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data1]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data2]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x000001988E560D10>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data2]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data3]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x000001988E560D10>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data3]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        },
        {
          "classname": "tests.test_rest_assured",
          "name": "test_conversational_memory_score[get_multiturn_data4]",
          "developer": "-",
          "test_description": "",
          "status": "Failed",
          "logs": "<ul style='list-style-type: none; padding: 0;'></ul>",
          "details": "Setup failed: self = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n>           return complexjson.loads(self.text, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\requests\\models.py:976: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346: in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x000001988E560D10>\ns = '<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\\n<html><head>\\n<title>408 Request Timeout</title>\\n</head><body>\\n...ient.</p>\\n<hr>\\n<address>Apache/2.4.52 (Ubuntu) Server at rahulshettyacademy.com Port 443</address>\\n</body></html>\\n'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <Coroutine test_conversational_memory_score[get_multiturn_data4]>\n\n    def setup(self) -> None:\n        runner_fixture_id = f\"_{self._loop_scope}_scoped_runner\"\n        if runner_fixture_id not in self.fixturenames:\n            self.fixturenames.append(runner_fixture_id)\n>       return super().setup()\n               ^^^^^^^^^^^^^^^\n\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:463: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:733: in pytest_fixture_setup\n    return (yield)\n            ^^^^^\nconftest.py:97: in get_multiturn_data\n    response_dict = ironman.get_rahul_shetty_llm_api_response(question_chathistory)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nutilities\\ironman.py:119: in get_rahul_shetty_llm_api_response\n    ).json()\n      ^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [408]>, kwargs = {}\n\n    def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n    \n        This may return a dictionary, list, etc. depending on what is in the response.\n    \n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n    \n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n    \n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n>           raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\nE           requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n.venv\\Lib\\site-packages\\requests\\models.py:980: JSONDecodeError",
          "functional_specifications": [],
          "categories": []
        }
      ]
    }
  ],
  "test_environment": "Development",
  "timestamp": "24 Nov 2025, 16:17",
  "img_url": "https://icon.icepanel.io/Technology/svg/pytest.svg",
  "test_status": "complete",
  "report_title": "pytest HTML Report",
  "category_count": {},
  "all_categories": []
}